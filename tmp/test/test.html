
<!DOCTYPE html>
<html lang="en">
<head>
<style>
.capture-image-td {
    width: 48vw;
    height: 1px;
}

img {
    /* max-width: 44vw;
    max-height: 90vh;
    position: fixed;
    top: 0;
    left: auto;
    right: auto; */
    width: inherit;
}

td {
    border: 1px solid black;
    padding: 1vw 1vw;
}

table {
    border-collapse: collapse;
    border: 1px solid black;
}

.capture-text-block-con {
    margin: 1vw;
    padding: 1vw;
    box-shadow: 0px 0px 5px 0px rgba(0, 0, 0, 0.75);
}

.paired-translate-con {
    margin: 1vw;
    padding: 1vw;
    box-shadow: 0px 0px 5px 0px rgba(0, 0, 0, 0.75);
}

p {
    margin-block-start: 0px;
    margin-block-end: 0px;
}

div {
    overflow: auto;
}

body {
    font-family: Arial, Helvetica, sans-serif;
    font-weight: lighter;
}

.capture-clip-div {
    width: 100%;
    height: fit-content;
}

.capture-text-con {
    height: max(20vh, 100%);
}

</style>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="styles.css" rel="stylesheet" type="text/css">
    <title>test</title>
</head>
<body>
    <table>
        <tr>
            <th>Capture</th>
            <th>Trans</th>
        </tr>
        <tr class="capture-row">
    <td class="capture-image-td">
        <div class="capture-clip-div">
            <img src="./images/4772759568.jpeg"/>
        </div>
    </td>
    <td class="capture-text-td">
        <div class="capture-tex-con">
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Conditional GAN with Discriminative Filter Generation for Text-to- Video Synthesis
                    </p>
                    <br />
                    <p class="trans-text">
                        テキストからビデオへの合成のための識別フィルター生成を備えた条件付き GAN
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Yogesh Balaji!
                    </p>
                    <br />
                    <p class="trans-text">
                        ヨゲシュ・バラジ！
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        *, Martin Rengiang Min”, Bing Bai?
                    </p>
                    <br />
                    <p class="trans-text">
                        *、マーティン・レンジャン・ミン」、ビンバイ？
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        , Rama Chellappa' and Hans Peter Graf?
                    </p>
                    <br />
                    <p class="trans-text">
                        、ラーマ・チェラッパ』とハンス・ペーター・グラフ？
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        ‘University of Maryland, College Park 2NEC Labs America - Princeton ybalaji@cs.umd.edu, {renqiang, bbai} @nec-labs.com, rama@umiacs.umd.edu, hpg@nec-labs.com
                    </p>
                    <br />
                    <p class="trans-text">
                        メリーランド大学カレッジパーク 2NEC Labs America - プリンストン ybalaji@cs.umd.edu、{renqiang、bbai} @nec-labs.com、rama@umiacs.umd.edu、hpg@nec-labs.com
                    </p>
                </div>
            </div>
        </div>
    </td>
</tr>
<tr class="capture-row">
    <td class="capture-image-td">
        <div class="capture-clip-div">
            <img src="./images/4772760272.jpeg"/>
        </div>
    </td>
    <td class="capture-text-td">
        <div class="capture-tex-con">
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Abstract
                    </p>
                    <br />
                    <p class="trans-text">
                        概要
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Developing conditional generative models for textto-video synthesis is an extremely challenging yet an important topic of research in machine learning.
                    </p>
                    <br />
                    <p class="trans-text">
                        テキストとビデオを合成するための条件付き生成モデルの開発は、非常に困難ですが、機械学習における研究の重要なトピックです。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In this work, we address this problem by introducing Text-Filter conditioning Generative Adversarial Network (TFGAN), a conditional GAN model with a novel multi-scale text-conditioning scheme that improves text-video associations.
                    </p>
                    <br />
                    <p class="trans-text">
                        この研究では、テキストとビデオの関連付けを改善する新しいマルチスケール テキスト コンディショニング スキームを備えた条件付き GAN モデルである、テキスト フィルター コンディショニング Generative Adversarial Network (TFGAN) を導入することで、この問題に対処します。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        By combining the proposed conditioning scheme with a deep GAN architecture, TFGAN generates high quality videos from text on challenging real-world video datasets.
                    </p>
                    <br />
                    <p class="trans-text">
                        提案された調整スキームとディープ GAN アーキテクチャを組み合わせることで、TFGAN は困難な現実世界のビデオ データセットのテキストから高品質のビデオを生成します。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In addition, we construct a synthetic dataset of text-conditioned moving shapes to systematically evaluate our conditioning scheme.
                    </p>
                    <br />
                    <p class="trans-text">
                        さらに、テキストで条件付けされた移動形状の合成データセットを構築して、条件付けスキームを体系的に評価します。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Extensive experiments demonstrate that TFGAN significantly outperforms existing approaches, and can also generate videos of novel categories not seen during training.
                    </p>
                    <br />
                    <p class="trans-text">
                        広範な実験により、TFGAN が既存のアプローチよりも大幅に優れたパフォーマンスを示し、トレーニング中には見られなかった新しいカテゴリのビデオも生成できることが実証されました。
                    </p>
                </div>
            </div>
        </div>
    </td>
</tr>
<tr class="capture-row">
    <td class="capture-image-td">
        <div class="capture-clip-div">
            <img src="./images/4772760464.jpeg"/>
        </div>
    </td>
    <td class="capture-text-td">
        <div class="capture-tex-con">
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        1 Introduction
                    </p>
                    <br />
                    <p class="trans-text">
                        1 はじめに
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Generative models have gained much interest in the research community over the last few years for unsupervised representation learning.
                    </p>
                    <br />
                    <p class="trans-text">
                        生成モデルは、教師なし表現学習として、ここ数年研究コミュニティで大きな関心を集めています。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] have been one of the most successful generative models till date.
                    </p>
                    <br />
                    <p class="trans-text">
                        Generative Adversarial Networks (GAN) [Goodfellow et al.、2014] は、これまでで最も成功した生成モデルの 1 つです。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Following its introduction in 2014, significant progress has been made towards improving the stability, quality and the diversity of the generated images [Salimans et al., 2016][Karras et al., 2017].
                    </p>
                    <br />
                    <p class="trans-text">
                        2014 年の導入以来、生成される画像の安定性、品質、多様性の向上に向けて大きな進歩が見られました [Salimans et al., 2016][Karras et al., 2017]。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        While GANs have been successful in the image domain, recent efforts have extended it to other modalities such as text [Wang et al., 2018a], graphs [Wang et al., 2018b], etc.
                    </p>
                    <br />
                    <p class="trans-text">
                        GAN は画像領域で成功していますが、最近の取り組みにより、テキスト [Wang et al., 2018a]、グラフ [Wang et al., 2018b] などの他のモダリティに拡張されています。
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In this work, we focus on the less studied domain of videos.
                    </p>
                    <br />
                    <p class="trans-text">
                        この研究では、ビデオというあまり研究されていない領域に焦点を当てます。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Generating videos are much harder than images because the additional temporal dimension makes generated data extremely high dimensional, and the generated sequences must be both photo-realistically diverse and temporally consistent.
                    </p>
                    <br />
                    <p class="trans-text">
                        ビデオの生成は画像よりもはるかに困難です。これは、追加の時間的次元によって生成されるデータが非常に高次元になり、生成されるシーケンスがフォトリアリスティックに多様であり、時間的に一貫している必要があるためです。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        We tackle the problem of text-conditioned video synthesis where the input is a text description and the goal is to synthesize a video corresponding to the input text.
                    </p>
                    <br />
                    <p class="trans-text">
                        私たちは、入力がテキスト説明であり、目標は入力テキストに対応するビデオを合成する、テキスト条件付きビデオ合成の問題に取り組みます。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        This problem has many potential applications, some of which include
                    </p>
                    <br />
                    <p class="trans-text">
                        この問題には多くの潜在的な応用例があり、その中には次のようなものがあります。
                    </p>
                </div>
            </div>
        </div>
    </td>
</tr>
<tr class="capture-row">
    <td class="capture-image-td">
        <div class="capture-clip-div">
            <img src="./images/4772760592.jpeg"/>
        </div>
    </td>
    <td class="capture-text-td">
        <div class="capture-tex-con">
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        generating synthetic data for machine learning tasks, domain adaptation, multimedia applications, etc.
                    </p>
                    <br />
                    <p class="trans-text">
                        機械学習タスク、ドメイン適応、マルチメディア アプリケーションなどのための合成データの生成
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Two recent works that address the problem of textconditioned video generation include [Li et al., 2018] and [Pan et al., 2017].
                    </p>
                    <br />
                    <p class="trans-text">
                        テキスト条件付きビデオ生成の問題に対処する 2 つの最近の研究には、[Li et al., 2018] と [Pan et al., 2017] があります。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Both these methods are variants of conditional GAN applied to the video data.
                    </p>
                    <br />
                    <p class="trans-text">
                        これらの方法は両方とも、ビデオ データに適用される条件付き GAN の変形です。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In spite of some successes, they have the following limitations: (1) They employ 3D transposed convolution layers in the generator network, which constrains them to only produce fixed-length videos.
                    </p>
                    <br />
                    <p class="trans-text">
                        いくつかの成功にもかかわらず、これらには次の制限があります。 (1) 生成ネットワークで 3D 転置畳み込み層を採用しているため、固定長のビデオのみを生成するように制約されます。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        (2) Their models are trained on low-resolution videos - results are shown only at a 64 x 64 resolution.
                    </p>
                    <br />
                    <p class="trans-text">
                        (2) モデルは低解像度のビデオでトレーニングされています。結果は 64 x 64 の解像度でのみ表示されます。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        (3) Text conditioning is performed using a simple concatenation of video and text features in the discriminator: Such a conditioning scheme may perform well on certain datasets, but maybe inadequate for capturing rich video-text variations.
                    </p>
                    <br />
                    <p class="trans-text">
                        (3) テキスト コンディショニングは、ディスクリミネーターでビデオとテキストの特徴を単純に連結して実行されます。このようなコンディショニング スキームは、特定のデータセットではうまく機能する可能性がありますが、豊富なビデオとテキストのバリエーションをキャプチャするには不十分である可能性があります。
                    </p>
                </div>
            </div>
        </div>
    </td>
</tr>
<tr class="capture-row">
    <td class="capture-image-td">
        <div class="capture-clip-div">
            <img src="./images/4772760400.jpeg"/>
        </div>
    </td>
    <td class="capture-text-td">
        <div class="capture-tex-con">
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In this work, we aim to address all the concerns above.
                    </p>
                    <br />
                    <p class="trans-text">
                        この取り組みでは、上記のすべての懸念に対処することを目的としています。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        First, to model videos of varying lengths, we use a recurrent neural network in the latent space and employ a shared frame generator network similar to [Tulyakov et al., 2018].
                    </p>
                    <br />
                    <p class="trans-text">
                        まず、さまざまな長さのビデオをモデル化するために、潜在空間でリカレント ニューラル ネットワークを使用し、[Tulyakov et al., 2018] と同様の共有フレーム ジェネレーター ネットワークを採用します。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Second, we present a model for generating higher-resolution videos by using a ResNet-style architecture in the generator and the discriminator network.
                    </p>
                    <br />
                    <p class="trans-text">
                        次に、ジェネレーターとディスクリミネーター ネットワークで ResNet スタイルのアーキテクチャを使用して、高解像度のビデオを生成するモデルを紹介します。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Third, we propose a new multi-scale text-conditioning scheme based on discriminative convolutional filter generation to strengthen the associations between the conditioned text and the generated video.
                    </p>
                    <br />
                    <p class="trans-text">
                        第三に、条件付きテキストと生成されたビデオの間の関連性を強化するために、識別畳み込みフィルタ生成に基づく新しいマルチスケール テキスト条件付けスキームを提案します。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        We call our model Text-Filter conditioning GAN (TFGAN).
                    </p>
                    <br />
                    <p class="trans-text">
                        このモデルをテキスト フィルター コンディショニング GAN (TFGAN) と呼びます。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Finally, we construct a synthetic moving shapes dataset to extensively evaluate the effectiveness of the proposed conditioning scheme.
                    </p>
                    <br />
                    <p class="trans-text">
                        最後に、提案された調整スキームの有効性を広範囲に評価するために、合成移動形状データセットを構築します。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Constructing this synthetic dataset is extremely useful as it captures rich text-video variations that are lacking in the datasets currently used for text-to-video synthesis [Li et al., 2018; Pan et al., 2017].
                    </p>
                    <br />
                    <p class="trans-text">
                        この合成データセットの構築は、テキストとビデオの合成に現在使用されているデータセットには欠けている豊富なテキストとビデオのバリエーションをキャプチャするため、非常に役立ちます [Li et al., 2018; Li et al., 2018; Pan et al.、2017]。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        We demonstrate the effectiveness of our approach on (1) real-world datasets such as Kinetics Human Action dataset [Kay et al., 2017] that has complex videos with high diversity but has relatively simple text-video variations, and (2) synthetic dataset that has simple video dynamics but models complex text-video associations.
                    </p>
                    <br />
                    <p class="trans-text">
                        我々は、(1) 多様性の高い複雑なビデオを含むが比較的単純なテキストとビデオのバリエーションを持つ Kinetics Human Action データセット [Kay et al., 2017] などの実世界のデータセット、および (2) 合成データに対するアプローチの有効性を実証します。シンプルなビデオ ダイナミクスを持ちながら、複雑なテキストとビデオの関連付けをモデル化したデータセット。
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In summary, our contributions in this work are as follows: (i) A new conditional GAN with an effective multi-scale text-conditioning scheme based on discriminative convolutional filter generation is proposed; (ii) A synthetic dataset for studying text conditioning in video generation is presented; (iii) A framework for generating complex video sequences and capturing rich text-video variations is presented.
                    </p>
                    <br />
                    <p class="trans-text">
                        要約すると、この研究における私たちの貢献は次のとおりです。(i) 識別畳み込みフィルタ生成に基づく効果的なマルチスケール テキスト条件付けスキームを備えた新しい条件付き GAN が提案されます。 (ii) ビデオ生成におけるテキストコンディショニングを研究するための合成データセットが提示されます。 (iii) 複雑なビデオ シーケンスを生成し、豊富なテキストとビデオのバリエーションをキャプチャするためのフレームワークが提示されます。
                    </p>
                </div>
            </div>
        </div>
    </td>
</tr>
    </table>
</body>
</html>
