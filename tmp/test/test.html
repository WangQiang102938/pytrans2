
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="styles.css" rel="stylesheet" type="text/css">
    <title>test</title>
</head>
<body>
    <table>
        <tr>
            <th>Capture</th>
            <th>Trans</th>
        </tr>
        <tr class="capture-row">
    <td class="capture-image-td">
        <div class="capture-clip-div">
            <img src="./images/5822542032.jpeg"/>
        </div>
    </td>
    <td class="capture-text-td">
        <div class="capture-tex-con">
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Abstract
                    </p>
                    <br />
                    <p class="trans-text">
                        抽象的
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Developing conditional generative models for textto-video synthesis is an extremely challenging yet an important topic of research in machine learning.
                    </p>
                    <br />
                    <p class="trans-text">
                        开发用于文本到视频合成的条件生成模型是一个极具挑战性但也是机器学习研究的重要课题。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In this work, we address this problem by introducing Text-Filter conditioning Generative Adversarial Network (TFGAN), a conditional GAN model with a novel multi-scale text-conditioning scheme that improves text-video associations.
                    </p>
                    <br />
                    <p class="trans-text">
                        在这项工作中，我们通过引入文本过滤器调节生成对抗网络 (TFGAN) 来解决这个问题，TFGAN 是一种条件 GAN 模型，具有新颖的多尺度文本调节方案，可改善文本-视频关联。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        By combining the proposed conditioning scheme with a deep GAN architecture, TFGAN generates high quality videos from text on challenging real-world video datasets.
                    </p>
                    <br />
                    <p class="trans-text">
                        通过将所提出的调节方案与深度 GAN 架构相结合，TFGAN 从具有挑战性的真实世界视频数据集上的文本生成高质量视频。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In addition, we construct a synthetic dataset of text-conditioned moving shapes to systematically evaluate our conditioning scheme.
                    </p>
                    <br />
                    <p class="trans-text">
                        此外，我们构建了一个文本条件移动形状的综合数据集，以系统地评估我们的条件方案。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Extensive experiments demonstrate that TFGAN significantly outperforms existing approaches, and can also generate videos of novel categories not seen during training.
                    </p>
                    <br />
                    <p class="trans-text">
                        大量实验表明，TFGAN 明显优于现有方法，并且还可以生成训练期间未见过的新类别视频。
                    </p>
                </div>
            </div>
        </div>
    </td>
</tr>
<tr class="capture-row">
    <td class="capture-image-td">
        <div class="capture-clip-div">
            <img src="./images/5822545248.jpeg"/>
        </div>
    </td>
    <td class="capture-text-td">
        <div class="capture-tex-con">
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        1 Introduction
                    </p>
                    <br />
                    <p class="trans-text">
                        1 简介
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Generative models have gained much interest in the research community over the last few years for unsupervised representation learning.
                    </p>
                    <br />
                    <p class="trans-text">
                        在过去几年中，生成模型在无监督表示学习方面引起了研究界的极大兴趣。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] have been one of the most successful generative models till date.
                    </p>
                    <br />
                    <p class="trans-text">
                        生成对抗网络 (GAN) [Goodfellow 等人，2014 年] 是迄今为止最成功的生成模型之一。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Following its introduction in 2014, significant progress has been made towards improving the stability, quality and the diversity of the generated images [Salimans er al., 2016][Karras et al., 2017].
                    </p>
                    <br />
                    <p class="trans-text">
                        自 2014 年推出以来，在提高生成图像的稳定性、质量和多样性方面取得了重大进展 [Salimans 等人，2016 年][Karras 等人，2017 年]。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        While GANs have been successful in the image domain, recent efforts have extended it to other modalities such as text [Wang et al., 2018a], graphs [Wang et al., 2018b], etc.
                    </p>
                    <br />
                    <p class="trans-text">
                        虽然 GAN 在图像领域取得了成功，但最近的努力已将其扩展到其他模式，例如文本 [Wang et al., 2018a]、图形 [Wang et al., 2018b] 等。
                    </p>
                </div>
            </div>
            <div class="capture-text-block-con">
                <div class="paired-translate-con">
                    <p class="origin-text">
                        In this work, we focus on the less studied domain of videos.
                    </p>
                    <br />
                    <p class="trans-text">
                        在这项工作中，我们专注于研究较少的视频领域。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        Generating videos are much harder than images because the additional temporal dimension makes generated data extremely high dimensional, and the generated sequences must be both photo-realistically diverse and temporally consistent.
                    </p>
                    <br />
                    <p class="trans-text">
                        生成视频比图像难得多，因为额外的时间维度使生成的数据具有极高的维度，并且生成的序列必须具有照片般逼真的多样性和时间一致性。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        We tackle the problem of text-conditioned video synthesis where the input is a text description and the goal is to synthesize a video corresponding to the input text.
                    </p>
                    <br />
                    <p class="trans-text">
                        我们解决了文本条件视频合成的问题，其中输入是文本描述，目标是合成与输入文本相对应的视频。
                    </p>
                </div>
                <div class="paired-translate-con">
                    <p class="origin-text">
                        This problem has many potential applications, some of which include
                    </p>
                    <br />
                    <p class="trans-text">
                        这个问题有很多潜在的应用，其中一些包括
                    </p>
                </div>
            </div>
        </div>
    </td>
</tr>
    </table>
</body>
</html>
